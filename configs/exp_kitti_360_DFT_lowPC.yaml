defaults:                       ### This Code is to run for debugging purpose, so the model parameters are very much shortened. For normal training, please follow "exp_kitti_360_DFT.yaml"
    - default
    - data: kitti_360_DFT
    - _self_

name: "kitti_360_DFT"
model: "bts"                    # model: "bts"/ "bts_overfit"    # TODO: bts_~~ for debug ## need to modify for debugging for different bts model ## replace en-decoder for trained one
output_path: "out/kitti_360"

num_epochs: 100000
#validate_every: 5000
batch_size: 4                  # default: 16
visualize_every: 1000

save_best:
    metric: abs_rel             # The model that has the smallest absolute relative error will be saved.
    sign: -1                    # The sign is negative, meaning that the model with the smallest abs_rel is the best

data:
    data_fc: 2
    image_size: [192, 640]
    color_aug: true
    is_preprocessed: true
    fisheye_rotation: [0, -15]

resume_from: "/storage/user/hank/BehindTheScenes/out/kitti_360/pretrained/training-checkpoint.pt"

model_conf:
    arch: "MVBTSNet"            # constructor for MVBTSNet
    use_code: true
    prediction_mode: default
    num_multiviews: 3           # number of multi views to aggregate. Default: 2
    DFT_flag: true
    feature_pad: true
    dropout_views_rate: 0.5     # 0==turn off: let model to learn robustness by dropping out sampled_features randomly
    num_layers: 4               # num of encoder layers, GeoNeRF's Transformer: default: 4
    d_model: 103                # num channels for each token from the feature. It could be adjustable unless it's prime number          ## complexity analysis (maybe to be done in linear for efficiency)
    att_feat: 32                # number of dim of embedding FFlayer (to reduce the dim of feature embedding and make divisible by num of heads)
    nhead: 4
    DFEnlayer: true             # Use Density Field Transformer Encoder layer
    AE: false                    # GeoNeRF's AutoEncoder for view-independent tokens (ablation)

    code:
        num_freqs: 6
        freq_factor: 1.5
        include_input: true

    encoder:
        type: "monodepth2"      # could be DFT version when encoder needs to be trained
        freeze: true            # freezing encoder to train the transformer
        pretrained: true
        resnet_layers: 50
        num_ch_dec: [32,32,64,128,256]
        d_out: 64               # c.f. paper D.1. output channel dimension 64 as hparam for the best result

    mlp_coarse:
        type : "resnet"
        n_blocks : 0
        d_hidden : 64

    mlp_fine:
        type : "empty"
        n_blocks : 1
        d_hidden : 128

    z_near: 3
    z_far: 80
    inv_z: true

    n_frames_encoder: 1
    n_frames_render: 2
    frame_sample_mode: kitti360-mono

    sample_mode: patch
    patch_size: 8                       # => possible number of points along a ray in total: 4096 * 64 = 262144
    ray_batch_size: 1024                # default: 4096 #_sampled points in an array from one single view. Note: assert ray_batch_size % (self.patch_size_x * self.patch_size_y
    flip_augmentation: true             # data augmentation

    learn_empty: false
    code_mode: z

loss:
    criterion: "l1+ssim"                # option: "l1+ssim+geo+cls"
    invalid_policy: weight_guided
    lambda_edge_aware_smoothness: 0.001

scheduler:
    type: step
    step_size: 120000
    gamma: 0.1

renderer:
    n_coarse : 32                       # default: 64, batch size == num of sampling a patch
    n_fine : 0
    n_fine_depth : 0
    depth_std : 1.0
    sched : []
    white_bkgd : false
    lindisp: true
    hard_alpha_cap: true

#This YAML configuration file specifies settings for training a BTSNet neural network model on the KITTI 360 dataset. It outlines parameters for various aspects of the training, including data processing, model configuration, loss functions, and the training schedule. Here are some key details:
#General Settings: The model to be used is BTSNet. The output from this training run will be stored in the directory "out/kitti_360". The training will run for 25 epochs with a batch size of 16.
#Data Parameters: The data configuration specifies that the model will use color augmentation and preprocessing. The images will be resized to a size of 192x640. The fisheye_rotation parameter is set to [0, -15] degrees, suggesting some form of geometric transformation on the input data.
#Model Configuration: The main architecture of the model is BTSNet. The encoder type is "monodepth2", a popular model for depth estimation, with 50 ResNet layers. The mlp_coarse and mlp_fine parameters dictate the architecture of the multi-layer perceptron (MLP) used in the coarse and fine stages of BTSNet respectively. The MLP for the coarse stage is a resnet with 0 blocks and 64 hidden dimensions, while the MLP for the fine stage is an "empty" type with 1 block and 128 hidden dimensions.
#Loss Parameters: The loss function used is a combination of L1 and SSIM (Structural Similarity Index Measure) losses. The policy for handling invalid values in the loss computation is "weight_guided". The lambda_edge_aware_smoothness is a weight parameter for the edge-aware smoothness loss term, which encourages the model to produce depth maps with smooth transitions except at the edges of objects.
#Scheduler Parameters: The scheduler type is "step", indicating that the learning rate will be decreased at certain intervals (step size of 120000) by a factor of 0.1.
#Renderer Parameters: The renderer will use 64 coarse samples, and no fine samples. Other parameters include the standard deviation for the depth (depth_std), a flag indicating whether to use linearized disparity (lindisp), and a flag for a hard alpha cap.
#These settings provide an overall blueprint for training the BTSNet model on the specified dataset.
